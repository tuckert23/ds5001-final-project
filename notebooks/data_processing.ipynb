{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Data Processing\n",
    "## DS 5001\n",
    "### Author: Taylor Tucker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# SA\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# PCA\n",
    "from scipy.linalg import eigh as eig\n",
    "\n",
    "# LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "This data was collected from [Constellate](https://constellate.org). Please see the `manifest.txt` file for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = [\"text_num\", \"paragraph_num\", \"sentence_num\", \"token_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/sample/articles.jsonl\"\n",
    "\n",
    "# Open the JSONL file and save it's lines to a list\n",
    "with open(data_path, 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "\n",
    "texts = []\n",
    "text_ids = []\n",
    "page_ids = []\n",
    "titles = []\n",
    "years = []\n",
    "months = []\n",
    "days = []\n",
    "authors = []\n",
    "\n",
    "for text_num, json_str in enumerate(json_list):\n",
    "    result = json.loads(json_str)\n",
    "\n",
    "    # If the JSON file has the text, the text isn't empty, and the language is English\n",
    "    if \"fullText\" in result.keys() and result[\"fullText\"] != [] and result[\"language\"] == [\"eng\"]:\n",
    "        # For each page in the text\n",
    "        for page_num, text_str in enumerate(result[\"fullText\"]):\n",
    "            texts.append(result[\"fullText\"][page_num])\n",
    "            page_ids.append(page_num + 1)   # 1 indexing\n",
    "            text_ids.append(text_num + 1)   # 1 indexing\n",
    "\n",
    "            # LIB data\n",
    "            titles.append(result[\"title\"])\n",
    "            authors.append(\", \".join(result[\"creator\"]))  # create string of authors\n",
    "\n",
    "            date = result[\"datePublished\"].split(\"-\")\n",
    "            years.append(date[0])\n",
    "            months.append(date[1])\n",
    "            days.append(date[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from information\n",
    "data = {\"text_num\": text_ids,\n",
    "        \"title\": titles,\n",
    "        \"author\": authors,\n",
    "        \"pub_year\": years,\n",
    "        \"pub_mon\": months,\n",
    "        \"pub_day\": days,\n",
    "        \"page_num\": page_ids,\n",
    "        \"text_str\": texts,\n",
    "        }\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LIB table\n",
    "\n",
    "Using the metadata from the source files, I create the `LIB` table and save it to `LIB.csv` in the data/processed directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove text and page ids from data, delete duplicates.\n",
    "LIB = data.copy().drop([\"page_num\", \"text_str\"], axis=1).drop_duplicates().reset_index().drop(\"index\", axis=1)\n",
    "LIB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = LIB.sort_values(\"pub_year\").reset_index().drop(\"index\", axis=1)\n",
    "LIB[\"text_num\"] = [i + 1 for i in range(len(LIB[\"text_num\"].unique()))]\n",
    "LIB = LIB.set_index(\"text_num\")\n",
    "LIB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.to_csv(\"../data/processed/LIB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.merge(LIB.reset_index()[[\"text_num\", \"title\"]], on=\"title\").sort_values([\"pub_year\", \"page_num\"]).drop(\"text_num_x\", axis=1).rename(columns={\"text_num_y\":\"text_num\"}).reset_index().drop(\"index\", axis=1)\n",
    "data_new  = data_new[[\"text_num\", 'title', 'author', 'pub_year', 'pub_mon', 'pub_day', 'page_num', 'text_str']]\n",
    "data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I: Convert to F1\n",
    "\n",
    "In order to convert the data to F1 format, I need to reduce the data to minimum discursive units (i.e. tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the useful info for doc table\n",
    "\n",
    "data = data_new.drop([\"title\", \"author\", \"pub_year\", \"pub_mon\", \"pub_day\"], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text across pages since pages aren't really a level of discourse\n",
    "df_full_articles = data.groupby(\"text_num\")[\"text_str\"].apply(lambda s: \"\\n\".join(s)).to_frame()\n",
    "df_full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC = LIB.merge(df_full_articles, on=\"text_num\").reset_index()\n",
    "DOC.to_csv(\"../data/processed/DOC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split now by paragraphs\n",
    "df_paragraphs = df_full_articles[\"text_str\"].str.split(r\"\\n\\n+\", expand=True).stack().to_frame().rename(columns={0:\"paragraph_str\"})\n",
    "df_paragraphs.index.names = OHCO[:2]\n",
    "df_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up paragraphs\n",
    "\n",
    "df_paragraphs[\"paragraph_str\"] = df_paragraphs[\"paragraph_str\"].str.replace(r'\\n', ' ').str.strip() # Replace newlines\n",
    "df_paragraphs = df_paragraphs[~df_paragraphs[\"paragraph_str\"].str.match(r'^\\s*$')]   # Filter whitespace paragraphs\n",
    "df_paragraphs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a sentence-level splitting\n",
    "\n",
    "# Manual Creation\n",
    "# df_sentences = df_paragraphs[\"paragraph_str\"].str.split(r'[.?!;:\"]+', expand=True).stack().to_frame().rename(columns={0:\"sentence_str\"}) # Split on punctuation\n",
    "# df_sentences.index.names = OHCO[:3] # Add sentence_num to index\n",
    "# df_sentences = df_sentences[~df_sentences[\"sentence_str\"].str.match(r'^\\s*$')]  # Remove blank sentences\n",
    "# df_sentences.head()\n",
    "\n",
    "# Using NLTK Sentence tokenizer\n",
    "df_sentences = df_paragraphs[\"paragraph_str\"].apply(lambda x: pd.Series(nltk.sent_tokenize(x))).stack().to_frame().rename(columns={0:\"sentence_str\"})\n",
    "df_sentences.index.names = OHCO[:3]\n",
    "df_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DOC table by tokenizing\n",
    "\n",
    "# Manual Creation\n",
    "# df_tokens = df_sentences[\"sentence_str\"].str.split(r\"[\\s',-]\", expand=True).stack().to_frame().rename(columns={0:\"token_str\"})\n",
    "# df_tokens.index.names = OHCO[:4]\n",
    "# df_tokens = df_tokens[~df_tokens[\"token_str\"].str.match(r\"^\\s*$\")]\n",
    "# df_tokens.head()\n",
    "\n",
    "# Using NLTK word tokenizer\n",
    "df_tokens = df_sentences[\"sentence_str\"].apply(lambda x: pd.Series(nltk.word_tokenize(x))).stack().to_frame().rename(columns={0:\"token_str\"})\n",
    "df_tokens.index.names = OHCO[:4]\n",
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = df_tokens.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `df_tokens` and thus `TOKEN` contains the data broken up by minimum discursive elements (tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II: Convert the collection into F2\n",
    "\n",
    "To convert the data to F2, I will create the `TOKEN` table by adding some NLP elements. I will also be creating a `VOCAB` table. Thus, I'll have the `TOKEN`, `DOC`, `VOCAB`, and `LIB` tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.i Create `TOKEN` Table by adding NLP elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag each word and get its POS\n",
    "TOKEN[\"pos_tuple\"] = TOKEN[\"token_str\"].apply(lambda x: tuple(nltk.pos_tag([str(x)])[0]))\n",
    "TOKEN[\"pos\"] = TOKEN[\"pos_tuple\"].apply(lambda x: x[1])\n",
    "TOKEN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN[\"term_str\"] = TOKEN[\"token_str\"].str.lower().replace(\"[\\W_]\", \"\")\n",
    "TOKEN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out weird mathematical characters\n",
    "TOKEN = TOKEN[~TOKEN[\"term_str\"].apply(lambda x: np.any([ord(x[i]) > 127 for i in range(len(x))]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out weird hexcodes by filtering only for word characters\n",
    "TOKEN = TOKEN[TOKEN['term_str'].str.match(r\"\\w+\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = TOKEN.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = TOKEN[~TOKEN[\"term_str\"].str.match(r\"\\d+\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN.to_csv(\"../data/processed/TOKEN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.ii Create `VOCAB` table using `TOKEN` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VOCAB table\n",
    "VOCAB = TOKEN[\"term_str\"].value_counts().to_frame().rename(columns={'index':'term_str', 'term_str':'n'}).sort_index().reset_index().rename(columns={'index':'term_str'})   # Counting each instance of each token\n",
    "VOCAB.index.name = 'term_id'\n",
    "VOCAB.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to denote if a term is a number\n",
    "VOCAB['num'] = VOCAB[\"term_str\"].str.match(\"\\d+\").astype('int') # making num col for numbers\n",
    "VOCAB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to denote stopwords\n",
    "swords = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "swords = swords.reset_index().set_index('term_str')\n",
    "swords.columns = ['dummy']\n",
    "swords.dummy = 1\n",
    "swords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB[\"stopword\"] = VOCAB[\"term_str\"].map(swords[\"dummy\"])\n",
    "VOCAB[\"stopword\"] = VOCAB[\"stopword\"].fillna(0).astype(\"int\")\n",
    "VOCAB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redundant, but adding POS tag to VOCAB table\n",
    "\n",
    "VOCAB[\"pos\"] = VOCAB[\"term_str\"].apply(lambda x: nltk.pos_tag([str(x)])[0][1])\n",
    "VOCAB.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_NO_SWORDS = VOCAB[VOCAB[\"stopword\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_NO_SWORDS.to_csv(\"../data/processed/VOCAB_STOP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.to_csv(\"../data/processed/VOCAB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB = VOCAB_NO_SWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III: Annotate and Convert to F3\n",
    "\n",
    "Since the data is already annotated with POS and term strings, we will simply add lemmas and sentiment to this data to convert to F3 format. I will first add the lemmas using the `WordNetLemmatizer` from `nltk`, and then I will create sentiment scores on the document level and associate them with the tokens from that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lemmas to TOKEN table\n",
    "TOKEN[\"term_str\"] = TOKEN[\"term_str\"].apply(str)\n",
    "TOKEN[\"lemma\"] = TOKEN[\"term_str\"].apply(lambda x: nltk.stem.WordNetLemmatizer().lemmatize(x))\n",
    "TOKEN.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DOC to implement sentiment analysis\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "DOC['sentiment'] = DOC[\"text_str\"].apply(lambda x: sa.polarity_scores(x)[\"compound\"])\n",
    "DOC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging sentiment with TOKEN table\n",
    "\n",
    "TOKEN = pd.merge(TOKEN.reset_index(), DOC[[\"text_num\", \"sentiment\"]], how=\"inner\", on=\"text_num\").set_index(\"text_num\").sort_index()\n",
    "TOKEN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN.to_csv(\"../data/processed/TOKEN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV: TFIDF and Convert to F4\n",
    "\n",
    "In this section, I will be calculating the article-level TF-IDF and adding the relevant data to the `VOCAB` and `TOKEN` tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out weird hexcodes by filtering only for word characters\n",
    "VOCAB = VOCAB[VOCAB['term_str'].str.match(r\"\\w+\")]\n",
    "VOCAB.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding term_rank to vocab column\n",
    "VOCAB = VOCAB.sort_values(\"count\", ascending=False).reset_index()   # Sort by counts\n",
    "VOCAB.index.name = \"term_rank\"  # Set the index to the term rank\n",
    "VOCAB = VOCAB.reset_index()\n",
    "VOCAB = VOCAB.set_index(\"term_id\")\n",
    "VOCAB[\"term_rank\"] = VOCAB[\"term_rank\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding term_id to TOKEN_table\n",
    "TOKEN[\"term_id\"] = TOKEN[\"term_str\"].map(VOCAB.reset_index().set_index(\"term_str\")[\"term_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN.to_csv(\"../data/processed/TOKEN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing TF-IDF at the article level\n",
    "BOW = TOKEN.groupby([\"text_num\", \"term_id\"])[\"term_id\"].count().to_frame().rename(columns={\"term_id\": \"n\"})\n",
    "BOW[\"c\"] = BOW[\"n\"].astype(\"bool\").astype(\"int\")\n",
    "BOW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DTM\n",
    "DTCM = BOW[\"n\"].unstack().fillna(0).astype(\"int\")\n",
    "DTCM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TF-IDF\n",
    "TF = (DTCM.T / DTCM.T.sum()).T\n",
    "\n",
    "df = DTCM[DTCM > 0].sum()\n",
    "n = DTCM.shape[0]\n",
    "IDF = np.log10(n / df)\n",
    "\n",
    "TFIDF = TF + IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF.to_csv(\"../data/processed/TFIDF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_TFIDF = pd.merge(VOCAB, TFIDF.T, on=\"term_id\")\n",
    "VOCAB_TFIDF[\"tfidf_sum\"] = TFIDF.T.sum(axis=1)\n",
    "VOCAB_TFIDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_TFIDF[['term_rank','term_str','pos','tfidf_sum']]\\\n",
    "    .sort_values('tfidf_sum', ascending=False).head(25).style.background_gradient(\"PuBuGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VOCAB_TFIDF[['term_rank','term_str','pos','tfidf_sum']]\\\n",
    "    .sort_values('tfidf_sum', ascending=False).head(20).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_TFIDF.to_csv(\"../data/processed/VOCAB_TFIDF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_TFIDF = pd.merge(TOKEN.reset_index(), VOCAB_TFIDF[\"tfidf_sum\"], on=\"term_id\").set_index(\"text_num\")\n",
    "TOKEN_TFIDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_TFIDF.to_csv(\"../data/processed/TOKEN_TFIDF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V: PCA, LCA, and word2vec and Convert to F5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_terms = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_TFIDF = VOCAB_TFIDF.sort_values(\"tfidf_sum\", ascending=True).head(n_terms).reset_index()[\"term_id\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_DTM = DTCM.loc[:, top_TFIDF]\n",
    "filtered_DTM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.i PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COV = filtered_DTM.cov()\n",
    "COV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting eigenvalues and eigenvectors\n",
    "eig_vals, eig_vecs = eig(COV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vecs_table = pd.DataFrame(eig_vecs, index=COV.index, columns=COV.index)\n",
    "eig_vecs_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val_table = pd.DataFrame(eig_vals, index=COV.index, columns=[\"eig_val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_pairs = eig_val_table.join(eig_vecs_table.T)\n",
    "eig_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_pairs[\"exp_var\"] = np.round((eig_pairs[\"eig_val\"] / eig_pairs[\"eig_val\"].sum())*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pcs = eig_pairs.sort_values(\"exp_var\", ascending=False).head(10).reset_index(drop=True)\n",
    "top_pcs.index.name = \"comp_id\"\n",
    "top_pcs.index = [f\"PC{i}\" for i in top_pcs.index.to_list()]\n",
    "top_pcs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = top_pcs[COV.index].T\n",
    "loadings.index.name = \"term_id\"\n",
    "loadings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings['term_str'] = loadings.apply(lambda x: VOCAB.loc[int(x.name)].term_str, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb0_pos = loadings.sort_values('PC0', ascending=True).head(5).term_str.to_list()#.term_str.str.cat(sep=' ')\n",
    "lb0_neg = loadings.sort_values('PC0', ascending=False).head(5).term_str.to_list()#.term_str.str.cat(sep=' ')\n",
    "lb1_pos = loadings.sort_values('PC1', ascending=True).head(5).term_str.to_list()#.term_str.str.cat(sep=' ')\n",
    "lb1_neg = loadings.sort_values('PC1', ascending=False).head(5).term_str.to_list()#.term_str.str.cat(sep=' ')\n",
    "lb2_pos = loadings.sort_values('PC2', ascending=True).head(5).term_str.to_list()#.term_str.str.cat(sep=' ')\n",
    "lb2_neg = loadings.sort_values('PC2', ascending=False).head(5).term_str.to_list()#.term_str.str.cat(sep=' ')\n",
    "lb3_pos = loadings.sort_values('PC3', ascending=True).head(5).term_str.to_list()#.term_str.str.cat(sep=' ')\n",
    "lb3_neg = loadings.sort_values('PC3', ascending=False).head(5).term_str.to_list()#.term_str.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Books PC0+', lb0_pos)\n",
    "print('Books PC0-', lb0_neg)\n",
    "print('Books PC1+', lb1_pos)\n",
    "print('Books PC1-', lb1_neg)\n",
    "print('Books PC2+', lb2_pos)\n",
    "print('Books PC2-', lb2_neg)\n",
    "print('Books PC3+', lb3_pos)\n",
    "print('Books PC3-', lb3_neg)\n",
    "\n",
    "\n",
    "print(pd.DataFrame({\"PC0+\": lb0_pos, \"PC0-\": lb0_neg, \"PC1+\": lb1_pos, \"PC1-\": lb1_neg, \n",
    "              \"PC2+\": lb2_pos, \"PC2-\": lb2_neg, \"PC3+\": lb3_pos, \"PC3-\": lb3_neg}).T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding loadings to TOKEN and VOCAB tables\n",
    "\n",
    "VOCAB_PCA = pd.merge(VOCAB_TFIDF, loadings.drop('term_str', axis=1), on=\"term_id\")\n",
    "VOCAB_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_PCA = pd.merge(TOKEN_TFIDF.reset_index(), loadings.drop(\"term_str\", axis=1), on=\"term_id\").set_index(\"text_num\")\n",
    "TOKEN_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_PCA.to_csv(\"../data/processed/TOKEN_PCA.csv\")\n",
    "VOCAB_PCA.to_csv(\"../data/processed/VOCAB_PCA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.ii LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLES = TOKEN[TOKEN[\"pos\"].str.match(r'^NNS?$')]\\\n",
    "    .groupby(OHCO[:1]).term_str\\\n",
    "    .apply(lambda x: ' '.join(x))\\\n",
    "    .to_frame()\\\n",
    "    .rename(columns={'term_str':'article_str'})\n",
    "ARTICLES.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector space model\n",
    "# Use sklearn's Count Vectorizer to convert our corpus of articles into a document-term vector space\n",
    "\n",
    "tfv = CountVectorizer(max_features=n_terms, stop_words=\"english\")\n",
    "tf = tfv.fit_transform(ARTICLES[\"article_str\"])\n",
    "TERMS = tfv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 40\n",
    "lda = LDA(n_components=20, random_state=1819)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Theta dataframe\n",
    "THETA = pd.DataFrame(lda.fit_transform(tf), index=ARTICLES.index)\n",
    "THETA.columns.name = \"topic_id\"\n",
    "THETA.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Phi dataframe\n",
    "PHI = pd.DataFrame(lda.components_, columns=TERMS)\n",
    "PHI.index.name = \"topic_id\"\n",
    "PHI.columns.name = \"term_str\"\n",
    "PHI = PHI.T\n",
    "PHI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking top ten words for each topic based on weight\n",
    "TOPICS = PHI.stack().to_frame().rename(columns={0:'weight'}).groupby('topic_id').apply(lambda x: x[\"weight\"].sort_values(ascending=False).head(10).reset_index().drop('topic_id', axis=1)[\"term_str\"])\n",
    "TOPICS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_topics = TOPICS.reset_index()[['topic_id', 0, 1, 2, 3, 4]]\n",
    "latex_topics.index.name = \"\"\n",
    "latex_topics.columns.name = \"\"\n",
    "print(latex_topics.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics are quite interesting. Topic 0 clearly seems to be related to research and data, while topic 1 appears to be related to astrophysics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the topic words into a sentence and printing the result\n",
    "TOPICS['label'] = TOPICS.apply(lambda x: str(x.name) + ' ' + ' '.join(x[:7].astype('str')), 1)\n",
    "TOPICS[[\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS['doc_weight_sum'] = THETA.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS.sort_values('doc_weight_sum', ascending=True).plot.barh(y='doc_weight_sum', x='label',title=\"Document Importance by DWS\", figsize=(5,10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS.to_csv('../data/processed/TOPICS.csv')\n",
    "THETA.to_csv('../data/processed/THETA.csv')\n",
    "PHI.to_csv('../data/processed/PHI.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.iii word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lists of words from token table per article\n",
    "article_corpora = TOKEN[~TOKEN[\"pos\"].str.match(\"NNPS?\")].groupby(\"text_num\")[\"term_str\"].apply(lambda x: x.to_list()).reset_index()[\"term_str\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Word2Vec model out of corpora\n",
    "article_model = Word2Vec(article_corpora, vector_size=256, window=8, min_count=10, workers=4, seed=1819)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(A, B, C, model, n=2):\n",
    "    try:\n",
    "        return model.wv.most_similar(positive=[B, C], negative=[A])[:n]\n",
    "    except KeyError as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_analogy(\"tool\", \"work\", \"computer\", article_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_analogy(\"good\", \"computer\", \"bad\", article_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_analogy(\"computer\", \"automation\", \"people\", article_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_analogy(\"digital\", \"computer\", \"analog\", article_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_analogy(\"computer\", \"smart\", \"people\", article_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_analogy(\"computer\", \"data\", \"people\", article_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building coords frame for TSNE plot\n",
    "coords = pd.DataFrame(index=range(len(article_model.wv.key_to_index)))\n",
    "coords[\"label\"] = [word for word in article_model.wv.key_to_index]\n",
    "coords[\"vector\"] = coords[\"label\"].apply(lambda x: article_model.wv.get_vector(x))\n",
    "coords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = pd.DataFrame(coords[\"vector\"].to_list(), columns=range(256))\n",
    "coords = coords.join(vec_df).drop(\"vector\", axis=1).set_index('label')\n",
    "coords.index.name = \"term_str\"\n",
    "coords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords.to_csv(\"../data/processed/coords.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes of our output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data = [TOKEN_TFIDF, VOCAB_TFIDF, DOC, LIB]\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"TOKEN_TFIDF:\", TOKEN_TFIDF.shape)\n",
    "print(\"VOCAB_TFIDF:\", VOCAB_TFIDF.shape)\n",
    "print(\"DOC:\", DOC.shape)\n",
    "print(\"LIB:\", LIB.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we used NLP techniques to convert the raw corpus into various useful tables, which have been saved into the directory `/data/processed/`. This data will be used in the file `data_exploration.ipynb`, as well as in the final report paper, to glean insights into the perception of the dawn of the digital age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
